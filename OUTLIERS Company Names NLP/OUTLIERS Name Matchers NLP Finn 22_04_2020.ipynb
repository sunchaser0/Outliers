{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from cleanco import cleanco\n",
    "import pandas as pd\n",
    "from metaphone import doublemetaphone\n",
    "import fuzzy\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have been using this as a sort of base-line matcher. For example, Metaphone turns words into phonetics and then this sequence matcher is used to determine similarity between those two matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "#Test to see : which of the two is more computationally effective and if they ever differ on normalized input\n",
    "\n",
    "#set a correlation tolerance\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Distances including Bag Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEditDistance(string_one,string_two,ngrams=1,jaro_winkler=False,jaccard=False,bag=False,cosine=False,longest_common_substring= False):\n",
    "    \n",
    "    solutions = {}\n",
    "    scores = []\n",
    "    \n",
    "    if jaro_winkler != False:\n",
    "        solution = textdistance.JaroWinkler(qval=ngrams).normalized_similarity(string_one,string_two)\n",
    "        solutions[\"jaro-winkler\"] = solution\n",
    "        \n",
    "        \n",
    "    if jaccard != False:\n",
    "        solution = textdistance.Jaccard(qval=ngrams).normalized_similarity(string_one,string_two)\n",
    "        solutions[\"jaccard\"] = solution\n",
    "    \n",
    "    if bag != False:\n",
    "        solution = textdistance.Bag(qval=ngrams).normalized_similarity(string_one,string_two)\n",
    "        solutions[\"bag\"] = solution\n",
    "            \n",
    "    if cosine != False:\n",
    "        solution = textdistance.Cosine(qval=ngrams).normalized_similarity(one,two)\n",
    "        solutions[\"cosine\"] = solution\n",
    "        \n",
    "    if longest_common_substring != False:\n",
    "        solution = textdistance.LCSStr(qval=ngrams).normalized_similarity(one,two)\n",
    "        solutions[\"LCSS\"] = solution\n",
    "#     print(solutions)\n",
    "    \n",
    "        \n",
    "    for key in solutions.keys():\n",
    "        scores.append(solutions[key])\n",
    "    \n",
    "    return round((sum(scores)/(len(scores))),2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein's Edit Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return \"The strings are {} edits away\".format(distance[row][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical Name Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_company_name(company_data_frame):\n",
    "    \n",
    "    company_data_frame.company_name_match = company_data_frame.company_name\n",
    "    # Convert to uppercase\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.str.upper()\n",
    "    # Remove commas\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.replace(',', '')\n",
    "    # Remove hyphens\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.replace(' - ', ' ')\n",
    "    # Remove text between parenthesis\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.replace(r\"\\(.*\\)\", \"\")\n",
    "    \n",
    "    #finn: seems to strip away last bracket rather than just pulling text out\n",
    "    \n",
    "    \n",
    "    # Remove apostrophes\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.replace(\"'\", \"\")\n",
    "    #\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.replace(' AND ', ' & ')\n",
    "    # Remove spaces in the begining/end\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.str.strip()\n",
    "    # Remove business entities extensions (1)\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.apply(\n",
    "        lambda x: cleanco(x).clean_name() if type(x) == str else x)\n",
    "    # Remove dots\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.str.replace('.', '')\n",
    "    # Remove business entities extensions (2) - after removing the dots\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.apply(\n",
    "        lambda x: cleanco(x).clean_name() if type(x) == str else x)\n",
    "    # Specific Polish to companies\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.str.replace('SP ZOO', '')\n",
    "    # Remove European country names from supplier names\n",
    "    countries = ['ALBANIA', 'ANDORRA', 'AUSTRIA', 'AZERBAIJAN', 'BELARUS', 'BELGIUM', 'BOSNIA AND HERZEGOVINA',\n",
    "                 'BULGARIA', 'CROATIA', 'CYPRUS', 'CZECH REPUBLIC', 'DENMARK', 'ESTONIA', 'FAROE ISLANDS', 'FINLAND',\n",
    "                 'FRANCE', 'BRITTANY', 'GERMANY', 'DEUTSCHLAND', 'GREECE', 'GUERNSEY (CHANNEL ISLANDS)', 'HUNGARY',\n",
    "                 'ICELAND', 'IRELAND', 'ISLE OF MAN', 'ITALY', 'JERSEY (CHANNEL ISLANDS)', 'LATVIA', 'LIECHTENSTEIN',\n",
    "                 'LITHUANIA', 'LUXEMBOURG', 'MACEDONIA', 'MALTA', 'MOLDOVA', 'MONACO', 'MONTENEGRO', 'NETHERLANDS',\n",
    "                 'NEDERLAND', 'HOLLAND', 'NORWAY', 'POLAND', 'POLSKA', 'PORTUGAL', 'ROMANIA', 'RUSSIA', 'SAN MARINO',\n",
    "                 'SERBIA', 'SLOVAKIA', 'SLOVENIA', 'SPAIN', 'SWEDEN', 'SWITZERLAND', 'TURKEY', 'UNITED KINGDOM', 'UK',\n",
    "                 'SCOTLAND', 'WALES', 'CORNWALL', 'NORTHERN IRELAND', 'UKRAINE', 'VATICAN CITY']\n",
    "    for country in countries:\n",
    "        company_data_frame.company_name_match = company_data_frame.company_name_match.apply(\n",
    "            lambda x: x.replace(country, '') if (type(x) == str and x.endswith(country)) else x)\n",
    "    # Remove multiple whitespace\n",
    "    company_data_frame.company_name_match = company_data_frame.company_name_match.replace('\\s{2,}', ' ')\n",
    "    return company_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token-based approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_matcher(name,name_to_match):\n",
    "    \n",
    "    #can include spell-checker here to see if this changes token ratio significantly.\n",
    "    #ask David before doing that \n",
    "    \n",
    "    token_counter = 0\n",
    "    \n",
    "    for word in name.split():\n",
    "        if word in name_to_match.split():\n",
    "            token_counter += 1\n",
    "        else:\n",
    "            token_counter -=1\n",
    "    \n",
    "    tokens_ratio = token_counter/len(name_to_match.split())\n",
    "    \n",
    "    print(token_counter)\n",
    "    print(f\"% {round(tokens_ratio,2)*100}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigrams Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://stackoverflow.com/questions/46198597/python-string-matching-exactly-equal-to-postgresql-similarity-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def find_ngrams(text: str, number: int=3) -> set:\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    returns a set of ngrams for the given string\n",
    "    :param text: the string to find ngrams for\n",
    "    :param number: the length the ngrams should be. defaults to 3 (trigrams)\n",
    "    :return: set of ngram strings\n",
    "    \"\"\"\n",
    "\n",
    "    if not text:\n",
    "        return set()\n",
    "\n",
    "    words = [f'  {x} ' for x in re.split(r'\\W+', text.lower()) if x.strip()]\n",
    "\n",
    "    ngrams = set()\n",
    "\n",
    "    for word in words:\n",
    "        for x in range(0, len(word) - number + 1):\n",
    "            ngrams.add(word[x:x+number])\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(text1: str, text2: str, number: int=3) -> float:\n",
    "    \"\"\"\n",
    "    Finds the similarity between 2 strings using ngrams.\n",
    "    0 being completely different strings, and 1 being equal strings\n",
    "    \"\"\"\n",
    "\n",
    "    ngrams1 = find_ngrams(text1, number)\n",
    "    ngrams2 = find_ngrams(text2, number)\n",
    "\n",
    "    num_unique = len(ngrams1 | ngrams2)\n",
    "    num_equal = len(ngrams1 & ngrams2)\n",
    "\n",
    "    return float(num_equal) / float(num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142857142857142"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(\"STARBUCKS SERVICES LIMITED\",\"STARBUCKS HOLDINGS LIMITED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used Soundex; Double Metaphone, with options to do it token by token or compare the whole thing using Sequence Matcher (though no option yet to do it token by token and then compare them using Sequence Matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other phonetic algorithms that have been noted to be have more success:\n",
    "Phonex\n",
    "Phonix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.informit.com/articles/article.aspx?p=1848528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I've been using an apparently slower library for Metaphone \n",
    "\n",
    "# >>> import fuzzy\n",
    "# >>> soundex = fuzzy.Soundex(4)\n",
    "# >>> soundex('fuzzy')\n",
    "# 'F200'\n",
    "# >>> dmeta = fuzzy.DMetaphone()\n",
    "# >>> dmeta('fuzzy')\n",
    "# ['FS', None]\n",
    "# >>> fuzzy.nysiis('fuzzy')\n",
    "# 'FASY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex_matcher(name,name_to_match,by_tokens = False):\n",
    "    \n",
    "    \n",
    "    soundex = fuzzy.Soundex(4)\n",
    "\n",
    "  \n",
    "\n",
    "   \n",
    "    \n",
    "    #this does a token-by-token method\n",
    "    \n",
    "    #this definitely seems more effective than spell checking...\n",
    "    \n",
    "    #this could be used within the other models for example a trigrams, consider a hierarchy processing\n",
    "    \n",
    "    if by_tokens == True:\n",
    "    \n",
    "        soundex_token_counter = 0\n",
    "\n",
    "        for word in name.split():\n",
    "\n",
    "\n",
    "\n",
    "            if soundex(word) in [soundex(piece) for piece in name_to_match.split()]:\n",
    "                soundex_token_counter += 1\n",
    "            else:\n",
    "                soundex_token_counter -= 1\n",
    "\n",
    "\n",
    "\n",
    "        soundex_tokens_ratio =  soundex_token_counter/len(name_to_match.split())\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"% token soundex : {round(soundex_tokens_ratio,2)*100}\")\n",
    "        \n",
    "    else:\n",
    "        #Could use any other sequence matcher here besides similar: \n",
    "        \n",
    "        def similar(a, b):\n",
    "            from difflib import SequenceMatcher\n",
    "            return SequenceMatcher(None, a, b).ratio()\n",
    "        \n",
    "        \n",
    "\n",
    "        print(f\"% non_token soundex {round(similar(soundex(name), soundex(name_to_match)),2)*100}\")\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nysiis_matcher(name,name_to_match,by_tokens = False):\n",
    "    \n",
    "\n",
    "  \n",
    "   \n",
    "    \n",
    "    #this does a token-by-token method\n",
    "    \n",
    "    #this definitely seems more effective than spell checking...\n",
    "    \n",
    "    #this could be used within the other models for example a trigrams, consider a hierarchy processing\n",
    "    \n",
    "    if by_tokens == True:\n",
    "    \n",
    "        nysiis_token_counter = 0\n",
    "\n",
    "        for word in name.split():\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if fuzzy.nysiis(word) in [fuzzy.nysiis(piece) for piece in name_to_match.split()]:\n",
    "                nysiis_token_counter += 1\n",
    "            else:\n",
    "                nysiis_token_counter -= 1\n",
    "\n",
    "\n",
    "\n",
    "        nysiis_tokens_ratio =  nysiis_token_counter/len(name_to_match.split())\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"% token nysiis : {round(nysiis_tokens_ratio,2)*100}\")\n",
    "        \n",
    "    else:\n",
    "        #Could use any other sequence matcher here besides similar: \n",
    "        \n",
    "        def similar(a, b):\n",
    "            from difflib import SequenceMatcher\n",
    "            return SequenceMatcher(None, a, b).ratio()\n",
    "        \n",
    "        \n",
    "\n",
    "        print(f\"% non_token nysiis {round(similar(fuzzy.nysiis(name), fuzzy.nysiis(name_to_match)),2)*100}\")\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metaphone_matcher(name,name_to_match,by_tokens = False):\n",
    "    \n",
    "    \n",
    "    \n",
    "    dmeta = fuzzy.DMetaphone()\n",
    "\n",
    "  \n",
    "   \n",
    "    \n",
    "    #this does a token-by-token method\n",
    "    \n",
    "    #this definitely seems more effective than spell checking...\n",
    "    \n",
    "    #this could be used within the other models for example a trigrams, consider a hierarchy processing\n",
    "    \n",
    "    if by_tokens == True:\n",
    "    \n",
    "        metaphone_token_counter = 0\n",
    "\n",
    "\n",
    "        for word in name.split():\n",
    "           \n",
    "\n",
    "            if dmeta(word) in [dmeta(piece) for piece in name_to_match.split()]:\n",
    "              \n",
    "                metaphone_token_counter += 1\n",
    "            else:\n",
    "                \n",
    "                metaphone_token_counter -= 1\n",
    "\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "        metaphone_tokens_ratio =  metaphone_token_counter/len(name_to_match.split())\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "        print(f\"% token metaphone: {round(metaphone_tokens_ratio,2)*100}\")\n",
    "    \n",
    "    else:\n",
    "        #Could use any other sequence matcher here besides similar: \n",
    "        \n",
    "        def similar(a, b):\n",
    "            from difflib import SequenceMatcher\n",
    "            return SequenceMatcher(None, a, b).ratio()\n",
    "       \n",
    "\n",
    "        print(f\"% non_token metaphone {round(similar(dmeta(name), dmeta(name_to_match)),2)*100}\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some papers on Name Matching Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper on personal-name matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### http://users.cecs.anu.edu.au/~Peter.Christen/publications/tr-cs-06-02.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It describes more methods as well as combination of methods:\n",
    "## Editex: Edit distance combined with Soundex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The paper tests many of these algorithms with the following conclusions:\n",
    "Bag Distance is the fatest, followed by n-grams.\n",
    "Jaro-Winker edit distances had very strong matching capabilities\n",
    "\"5. The longest common sub-string technique is suitable\n",
    "for unparsed names that might contain swapped words.\"\n",
    "\n",
    "\n",
    "Favours \"Calculating a similarity measure with respect to the\n",
    "length of the shorter string (Overlap coefficient)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
